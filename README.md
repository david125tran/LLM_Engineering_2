#  **LLM Engineering Playground** 

### Exploring the Frontier of AI

---

### **01: Cloud-Powered Conversations & Summaries** üó™ 
- **Objective:** Secure API credentials in .env and interact with OpenAI's API using Python.  Also exploring structured API responses & inspecting metadata from the LLM's response such as token usage & model information.  

---

### **02: Vision Models & Structured Prompting** üëÅÔ∏è  
- **Objective:** Use OpenAI‚Äôs multimodal (vision-capable) models to analyze and reason about images.  
- **Highlights:**  
  - Convert and encode images into Base64 for API calls.  
  - Perform **zero-shot visual recognition** and **chain-of-thought prompting** for image understanding.  
  - Apply **structured prompt engineering** to return outputs as clean JSON ‚Äî useful for downstream analysis (e.g., calorie estimation).  
  - End with a **sentiment analysis task** on financial text, demonstrating text-only reasoning alongside visual tasks.  

---

### **03: Real-Time AI Tutor with Gradio** üéì  
- **Objective:** Build an interactive AI Tutor interface using **Gradio** and **OpenAI‚Äôs GPT-4o-mini** model.  
- **Highlights:**  
  - Stream model responses in real time for a ‚Äútyping‚Äù effect using OpenAI‚Äôs `stream=True` parameter.  
  - Experiment with **system prompts** to shape model personality, tone, and level of detail.  

---

### **08: HuggingFace Hub, Open-Source LLMs, Quantization** üß†  
- **Objective:** Run open-source LLMs locally (on Colab GPU), quantize them for memory savings, ground them on real documents, and expose the whole workflow as an interactive app.
- **Highlights:**  
  - Log in to Hugging Face and load models onto the GPU using two approaches: `generate()` and `pipeline()`. 
  - Explore 4-bit quantization (`BitsAndBytesConfig` / `load_in_4bit=True`) to lower model weights to make it less memory intensive.  
  - Connect the LLM to a real **knowledge base (Google‚Äôs Q1 2025 earnings report)** and query it through a **Gradio web app** ‚Äî turning it into a document-aware Q&A system.  

---

### **09: HuggingFace Continued ‚Äì Datasets & DeepSeek Chain-of-Thought Classification** üß©  
- **Objective:** Extend the workflow to dataset extraction from Hugging Face, then deploy and analyze **DeepSeek-R1-Distill-Qwen-1.5B**, an open-source reasoning model that exposes its internal chain-of-thought logic in code.  
- **Highlights:**  
  - Fetch and preprocess text datasets from Hugging Face for NLP classification tasks (sentiment, topic, intent).  
  - Inspect the **model‚Äôs reasoning traces** ‚Äî i.e., its internal ‚Äúchain-of-thought‚Äù ‚Äî as it breaks down a question, evaluates possibilities, and then produces a final answer.  

#### **Why This Matters ‚Äî The DeepSeek Revolution** ‚öôÔ∏è  
DeepSeek‚Äôs new architecture (**released 10 months ago, Jan. 2025**) is fundamentally changing how LLMs approach reasoning.  
Unlike traditional transformer models that only produce an answer token-by-token, **DeepSeek-R1** was trained with **reinforcement learning over reasoning steps** ‚Äî the model *thinks out loud* internally before committing to an answer.  DeepSeek made their architecture & self-attention layers different than conventional methods to allow for this functionality. 

When distilled into smaller models like **Qwen-1.5B**, this process still leaves behind structured reasoning traces that you can observe directly in code output.  

#### **Why It‚Äôs Different from Conventional Models** üîç  
- Most open-source LLMs are *inference-only black boxes*‚Äîthey produce a result without revealing how it was derived.  
- DeepSeek‚Äôs line of models exposes **traceable reasoning paths**, making it possible to debug, study, and even fine-tune on *reasoning quality*, not just final accuracy.  
