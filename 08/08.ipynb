{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riL70o3UReg6"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------- Libraries ----------------------------------\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch pypdf gradio\n",
        "!pip install --upgrade transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # force restarts the Colab runtime\n"
      ],
      "metadata": {
        "id": "vikzevLqfG0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Imports ----------------------------------\n",
        "import gc\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "from huggingface_hub import login, notebook_login\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pypdf\n",
        "import requests\n",
        "import torch\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "DSmgoFoAUJaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Connection ----------------------------------\n",
        "hf_token = os.environ.get('HF_TOKEN') or userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"HuggingFace login successful.\")\n",
        "else:\n",
        "    print(\"HuggingFace token not found. Please set the HF_TOKEN environment variable or store it in Colab secrets.\")"
      ],
      "metadata": {
        "id": "PDi9PGkgUdTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Use GPU ----------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    # Set default device to GPU\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    print(\"PyTorch default device set to CUDA (GPU).\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Running on CPU\")"
      ],
      "metadata": {
        "id": "zB6hjOecW9cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Functions ----------------------------------\n",
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown in Colab/Jupyter.\"\"\"\n",
        "    display(Markdown(text))"
      ],
      "metadata": {
        "id": "mKwxg917XuLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## transformers - A Python library that provides a standardized way to download, load, and use models from the Hub with just a few lines of code. Key classes:\n",
        "- **pipeline()** - A high-level, easy-to-use abstraction for common tasks (like text generation, summarization). Great for quick tests and beginners.\n",
        "- **AutoTokenizer** - Automatically downloads the correct \"tokenizer\" for a model. A tokenizer converts human-readable text into numerical IDs the model understands.\n",
        "- **AutoModelFor...** - Automatically downloads the correct model architecture and pre-trained weights (e.g., AutoModelForCausalLM for text generation models like GPT, Llama, Gemma).\n",
        "- Other Libraries - HF also develops libraries like accelerate (for efficient loading/distributed training), datasets (for handling datasets), and evaluate (for model evaluation metrics).\n",
        "\n"
      ],
      "metadata": {
        "id": "4XYBPhK5kGO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Open Source LLM Pipeline ----------------------------------\n",
        "# Load a sentiment classifier model on financial news data\n",
        "# https://huggingface.co/ProsusAI/finbert\n",
        "pipe = pipeline(model = \"ProsusAI/finbert\")\n",
        "pipe(\"Apple lost 10 Million dollars today due to US tarrifs\")"
      ],
      "metadata": {
        "id": "HUmW5FpiX_-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The finbert model classified the string of text as negative with a high rating.\n",
        "[{'label': 'negative', 'score': 0.9706032276153564}]"
      ],
      "metadata": {
        "id": "ZV9-UDxyYsD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer for GPT-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# Encode text to token IDs\n",
        "tokens = tokenizer(\"Hello everyone and welcome to LLM and AI Agents Bootcamp\")\n",
        "print(f\"gpt2 tokenizer:         {tokens}\")\n",
        "print(f\"{tokens['input_ids']}\")\n",
        "\n",
        "# Load tokenizer for bert-base-uncased\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Encode text to token IDs\n",
        "tokens = tokenizer(\"Hello everyone and welcome to LLM and AI Agents Bootcamp\")\n",
        "print(f\"bert-base-uncased:     {tokens}\")\n",
        "print(f\"{tokens['input_ids']}\")\n",
        "\n",
        "# Load tokenizer for facebook/opt-125m\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "# Encode text to token IDs\n",
        "tokens = tokenizer(\"Hello everyone and welcome to LLM and AI Agents Bootcamp\")\n",
        "print(f\"facebook/opt-125m:     {tokens}\")\n",
        "print(f\"{tokens['input_ids']}\")"
      ],
      "metadata": {
        "id": "d0BGiu7yYmyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt2 tokenizer:         {'input_ids': [15496, 2506, 290, 7062, 284, 27140, 44, 290, 9552, 28295, 18892, 16544], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "[15496, 2506, 290, 7062, 284, 27140, 44, 290, 9552, 28295, 18892, 16544]\n",
        "\n",
        "bert-base-uncased:     {'input_ids': [101, 7592, 3071, 1998, 6160, 2000, 2222, 2213, 1998, 9932, 6074, 9573, 26468, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "[101, 7592, 3071, 1998, 6160, 2000, 2222, 2213, 1998, 9932, 6074, 9573, 26468, 102]\n",
        "\n",
        "facebook/opt-125m:     {'input_ids': [2, 31414, 961, 8, 2814, 7, 30536, 448, 8, 4687, 24854, 21524, 21669], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "[2, 31414, 961, 8, 2814, 7, 30536, 448, 8, 4687, 24854, 21524, 21669]\n",
        "\n",
        "\n",
        "Different tokenizers are better at different things.  Breaking up the tokens into less tokens that are longer makes the LLM respond faster because it has less tokens to process."
      ],
      "metadata": {
        "id": "7VBJfoSAY4NV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoModelForCausalLM - HuggingFace Transformers Library\n",
        "`AutoModelForCausalLM` - A Hugging Face class that automatically loads a pretrained model for causal (left-to-right) language modeling, such as GPT, LLaMA, or Gemma.\n",
        "Key Steps:\n",
        "- Choose a Model ID:\n",
        " Example - `google/gemma-2b-it` or `microsoft/Phi-3-mini-4k-instruct`\n",
        "- Load the Tokenizer - `Use AutoTokenizer.from_pretrained(model_id)` to get the specific tokenizer for that model.\n",
        "- Load the Model - Use `AutoModelForCausalLM.from_pretrained(...)` with crucial arguments:\n",
        "  - `model_id` - The identifier.\n",
        "  - `torch_dtype=torch.float16` (or `bfloat16`) - Loads the model using 16-bit floating point numbers instead of 32-bit, saving memory.\n",
        "  - `load_in_4bit=True` or `load_in_8bit=True` - This is quantization via bitsandbytes. It further reduces memory by representing model weights with fewer bits (4 or 8 instead of 16/32). Essential for free Colab! 4-bit saves more memory but might have a tiny impact on quality compared to 8-bit.\n",
        "  - `device_map=\"auto\"` - Tells accelerate to automatically figure out how to spread the model across available devices (primarily the GPU in our case).\n",
        " - Combine Tokenizer and Model (Optional but common): Using the pipeline function is often simpler for basic text generation. It handles tokenization, model inference, and decoding back to text for you.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8PG_AiAnjiBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Open Source LLM ----------------------------------\n",
        "# Load the model\n",
        "# model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "# model_id = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "\n",
        "# Create BitsAndBytesConfig for 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                         bnb_4bit_compute_dtype = torch.float16,  # or torch.bfloat16 if available\n",
        "                                         bnb_4bit_quant_type = \"nf4\",             # normal float 4 quantization\n",
        "                                         bnb_4bit_use_double_quant = True         # use nested quantization for more efficient memory usage\n",
        "                                         )\n",
        "\n",
        "# Load the model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config = quantization_config,\n",
        "                                             device_map = \"auto\",\n",
        "                                             trust_remote_code = True)"
      ],
      "metadata": {
        "id": "LK3SHFFTapTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a prompt\n",
        "prompt = \"Explain how Electric Vehicles work in a funny way!\""
      ],
      "metadata": {
        "id": "iGv6sX6veM6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Method 1 - Generate Response using `generate()` ----------------------------------\n",
        "# Method 1 is used when the end user wants to tweak how they're interacting with the LLM more\n",
        "\n",
        "# Let's encode the input first\n",
        "inputs = tokenizer(prompt, return_tensors = \"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1000)\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print_markdown(response)"
      ],
      "metadata": {
        "id": "dqznOqybgDR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain how Electric Vehicles work in a funny way! Imagine that each battery in an electric vehicle is like a bunch of balloons, and the motor is like a super-strong helium blower. When you plug in the vehicle, it's like inflating all those balloons at once, making the car vibrate and dance around like a balloon animal! The more balloons (or batteries) you have, the bigger and more bouncy the car becomes. And when you use the brakes, it's like squeezing the balloons to make them deflate slowly, giving the car a fun, wiggly ride home. So, every time you drive, it's like a giant, floating party balloon going for a spin! But don't worry, it's all powered by clean, quiet, and very funny helium gas instead of noisy, smelly gasoline. Isn't that a silly yet creative way to think about electric vehicles?"
      ],
      "metadata": {
        "id": "zVXtCTsnmb4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Method 2 - Generate Response using `pipeline()` ----------------------------------\n",
        "# Method 2 is used more when the end user wants a simple to use wrapper with less tweaking of the interaction with the LLM\n",
        "\n",
        "# The pipeline wraps tokenization, generation, and decoding\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                torch_dtype = \"auto\", # Match model dtype\n",
        "                device_map = \"auto\" # Ensure pipeline uses the same device mapping\n",
        "                )\n",
        "\n",
        "\n",
        "outputs = pipe(prompt,\n",
        "               max_new_tokens = 1000, # max_new_tokens limits the length of the generated response.\n",
        "               temperature = 1, # temperature controls randomness (lower = more focused).\n",
        "               )\n",
        "\n",
        "# Print the generated text\n",
        "print_markdown(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "BmHiMLLQmOub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain how Electric Vehicles work in a funny way! Imagine you have a bunch of toys, like little cars or trucks, and you want to make them run on electricity instead of gasoline. To do this, you need a special power plant, which we'll call the \"charger plant.\" This plant has magical glowing wires that light up when it's ready to give juice to your toy vehicles.\n",
        "\n",
        "Now, every vehicle needs a battery box, kind of like a big container for holding electricity. When you plug one of these vehicles into the charger plant, the glowing wires start shooting out tiny energy beams towards the vehicle's battery box. The beams travel through invisible super-speedy tunnels inside the vehicle, just like how you might imagine a superhero flying through the air!\n",
        "\n",
        "As the glowing beams reach the battery box, they turn the battery's magic switch (which is actually a small button) on. The vehicle starts humming and whirring, much like when you press a toy car's starting button but much faster.\n",
        "\n",
        "When the vehicle runs out of energy, its magic switch (button) turns off automatically, and the vehicle slows down until it stops. At this point, the glowing wires from the charger plant come back to the charger plant itself. You don't even need to think about it; it's all automatic!\n",
        "\n",
        "The best part? When the vehicle runs out of energy, it automatically tells the charger plant to refill its batteries with a new batch of glowing energy beams. It's like the vehicle is saying, \"I'm low on juice! Give me some more!\" So you don't even need to remember to change the batteries – the vehicle takes care of it all by itself.\n",
        "\n",
        "In the end, you have a bunch of fun, shiny electric vehicles that are easy to operate and take care of, making driving and having fun a breeze! Isn't that silly and exciting?\n"
      ],
      "metadata": {
        "id": "Fga9DBr1n7Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Extracting pdf Text ----------------------------------\n",
        "# --- Get the PDF File ---\n",
        "pdf_url = \"https://s206.q4cdn.com/479360582/files/doc_financials/2025/q1/2025q1-alphabet-earnings-release.pdf\"\n",
        "pdf_filename = \"google_earning_transcript.pdf\"\n",
        "pdf_path = Path(pdf_filename)\n",
        "\n",
        "# Download the file if it doesn't exist\n",
        "if not pdf_path.exists():\n",
        "    response = requests.get(pdf_url)\n",
        "    response.raise_for_status()  # Check for download errors\n",
        "    pdf_path.write_bytes(response.content)\n",
        "    print(f\"PDF downloaded successfully to {pdf_path}\")\n",
        "else:\n",
        "    print(f\"PDF file already exists at {pdf_path}\")\n",
        "\n",
        "\n",
        "# --- Read Text from PDF using pypdf ---\n",
        "pdf_text = \"\"\n",
        "\n",
        "print(f\"Reading text from {pdf_path}...\")\n",
        "reader = pypdf.PdfReader(pdf_path)\n",
        "num_pages = len(reader.pages)\n",
        "print(f\"PDF has {num_pages} pages.\")\n",
        "\n",
        "# Extract text from each page\n",
        "all_pages_text = []\n",
        "for i, page in enumerate(reader.pages):\n",
        "\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:  # Only add if text extraction was successful\n",
        "        all_pages_text.append(page_text)\n",
        "    # print(f\"Read page {i+1}/{num_pages}\") # Uncomment for progress\n",
        "\n",
        "# Join the text from all pages\n",
        "pdf_text = \"\\n\".join(all_pages_text)\n",
        "print(f\"Successfully extracted text. Total characters: {len(pdf_text)}\")\n",
        "\n",
        "# Display a small snippet of the PDF\n",
        "print(\"\\n--- Snippet of Extracted Text ---\")\n",
        "print_markdown(f\"{pdf_text[:1000]}\")"
      ],
      "metadata": {
        "id": "o5I7L4xDpao_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Pdf Reading Function ----------------------------------\n",
        "# Define a limit for the context length to avoid overwhelming the model\n",
        "MAX_CONTEXT_CHARS = 6000\n",
        "\n",
        "def answer_question_from_pdf(document_text, question, llm_pipeline):\n",
        "    \"\"\"\n",
        "    Answers a question based on the provided document text using the loaded LLM pipeline.\n",
        "\n",
        "    Args:\n",
        "        document_text (str): The text extracted from the PDF.\n",
        "        question (str): The user's question.\n",
        "        llm_pipeline (transformers.pipeline): The initialized text-generation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's generated answer.\n",
        "    \"\"\"\n",
        "    # Truncate context if necessary\n",
        "    if len(document_text) > MAX_CONTEXT_CHARS:\n",
        "        print(f\"Warning: Document text ({len(document_text)} chars) exceeds limit ({MAX_CONTEXT_CHARS} chars). Truncating.\")\n",
        "        context = document_text[:MAX_CONTEXT_CHARS] + \"...\"\n",
        "    else:\n",
        "        context = document_text\n",
        "\n",
        "    # Prompt Template\n",
        "    prompt_template = f\"\"\"<|system|>\n",
        "    You are an AI assistant. Answer the following question based *only* on the provided document text. If the answer is not found in the document, say \"The document does not contain information on this topic.\" Do not use any prior knowledge.\n",
        "\n",
        "    Document Text:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "    <|end|>\n",
        "    <|user|>\n",
        "    Question: {question}<|end|>\n",
        "    <|assistant|>\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    print(f\"\\n--- Generating Answer for: '{question}' ---\")\n",
        "\n",
        "    # Run Inference on the chosen model\n",
        "    outputs = llm_pipeline(prompt_template,\n",
        "                           max_new_tokens = 500,  # Limit answer length\n",
        "                           do_sample = True,\n",
        "                           temperature = 0.2,   # Lower temperature for more factual Q&A\n",
        "                           top_p = 0.9)\n",
        "\n",
        "    # Extract the answer\n",
        "    full_generated_text = outputs[0]['generated_text']\n",
        "    answer_start_index = full_generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "    raw_answer = full_generated_text[answer_start_index:].strip()\n",
        "\n",
        "    # Sometimes the model might still include parts of the prompt or trail off.\n",
        "    # Basic cleanup: Find the end-of-sequence token if possible, or just return raw.\n",
        "    # Phi-3 uses <|end|> or <|im_end|>\n",
        "    end_token = \"<|end|>\"\n",
        "    if end_token in raw_answer:\n",
        "            raw_answer = raw_answer.split(end_token)[0]\n",
        "\n",
        "    print(\"--- Generation Complete ---\")\n",
        "    return raw_answer"
      ],
      "metadata": {
        "id": "87eSnuREqM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "test_question = \"What is this document about?\"\n",
        "generated_answer = answer_question_from_pdf(pdf_text, test_question, pipe)\n",
        "\n",
        "print(\"\\nTest Question:\")\n",
        "print_markdown(f\"**Q:** {test_question}\")\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print_markdown(f\"**A:** {generated_answer}\")"
      ],
      "metadata": {
        "id": "8RFwfV7NqbDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Question:\n",
        "\n",
        "Q: What is this document about?\n",
        "\n",
        "\n",
        "Generated Answer:\n",
        "\n",
        "A: This document is about Alphabet Inc.'s (formerly Google) financial results for the first quarter of 2025, including revenues, operating income, net income, and other financial metrics. It also includes details on the company's segment operating results, dividend program, and stock repurchase authorization. The document provides a comprehensive overview of Alphabet's financial performance and strategic initiatives for the quarter. <|end>|\n"
      ],
      "metadata": {
        "id": "4Ojvi7IGqgoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Gradio App ----------------------------------\n",
        "available_models = {\n",
        "    \"Llama 3.2\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    \"Microsoft Phi-4 Mini\": \"microsoft/Phi-4-mini-instruct\",\n",
        "    \"Google Gemma 3\": \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "    }\n",
        "\n",
        "# --- Global State (or use gr.State in Blocks) ---\n",
        "# To keep track of the currently loaded model/pipeline\n",
        "current_model_id = None\n",
        "current_pipeline = None\n",
        "print(f\"Models available for selection: {list(available_models.keys())}\")\n",
        "\n",
        "\n",
        "# Define a function to Load/Switch Models\n",
        "def load_llm_model(model_name):\n",
        "    \"\"\"Loads the selected LLM, unloading the previous one.\"\"\"\n",
        "    global current_model_id, current_pipeline, tokenizer, model\n",
        "\n",
        "    new_model_id = available_models.get(model_name)\n",
        "    if not new_model_id:\n",
        "        return \"Invalid model selected.\", None  # Return error message and None pipeline\n",
        "\n",
        "    if new_model_id == current_model_id and current_pipeline is not None:\n",
        "        print(f\"Model {model_name} is already loaded.\")\n",
        "        # Indicate success but don't reload\n",
        "        return f\"{model_name} already loaded.\", current_pipeline\n",
        "\n",
        "    print(f\"Switching to model: {model_name} ({new_model_id})...\")\n",
        "\n",
        "    # Unload previous model (important for memory)\n",
        "    # Clear variables and run garbage collection\n",
        "    current_pipeline = None\n",
        "    if \"model\" in locals():\n",
        "        del model\n",
        "    if \"tokenizer\" in locals():\n",
        "        del tokenizer\n",
        "    if \"pipe\" in locals():\n",
        "        del pipe\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "\n",
        "    gc.collect()\n",
        "    print(\"Previous model unloaded (if any).\")\n",
        "\n",
        "    # --- Load the new model ---\n",
        "    loading_message = f\"Loading {model_name}...\"\n",
        "    try:\n",
        "        # Load Tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(new_model_id, trust_remote_code = True)\n",
        "\n",
        "        # Load Model (Quantized)\n",
        "        model = AutoModelForCausalLM.from_pretrained(new_model_id,\n",
        "                                                     torch_dtype = \"auto\",  # \"torch.float16\", # Or bfloat16 if available\n",
        "                                                     load_in_4bit = True,\n",
        "                                                     device_map = \"auto\",\n",
        "                                                     trust_remote_code = True)\n",
        "\n",
        "        # Create Pipeline\n",
        "        loaded_pipeline = pipeline(\n",
        "            \"text-generation\", model = model, tokenizer = tokenizer, torch_dtype = \"auto\", device_map = \"auto\")\n",
        "\n",
        "        print(f\"Model {model_name} loaded successfully!\")\n",
        "        current_model_id = new_model_id\n",
        "        current_pipeline = loaded_pipeline  # Update global state\n",
        "        # Use locals() or return values with gr.State for better Gradio practice\n",
        "        return f\"{model_name} loaded successfully!\", loaded_pipeline  # Status message and the pipeline object\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_name}: {e}\")\n",
        "        current_model_id = None\n",
        "        current_pipeline = None\n",
        "        return f\"Error loading {model_name}: {e}\", None  # Error message and None pipeline"
      ],
      "metadata": {
        "id": "mlYc8SMPqzCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to handle Q&A Submission ---\n",
        "def handle_submit(question):\n",
        "    \"\"\"Handles the user submitting a question.\"\"\"\n",
        "    if not current_pipeline:\n",
        "        return \"Error: No model is currently loaded. Please select a model.\"\n",
        "    if not pdf_text:\n",
        "        return \"Error: PDF text is not loaded. Please run Section 4.\"\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    print(f\"Handling submission for question: '{question}' using {current_model_id}\")\n",
        "    # Call the Q&A function defined in Section 5\n",
        "    answer = answer_question_from_pdf(pdf_text, question, current_pipeline)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "H8mczksKq-5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build Gradio Interface using Blocks ---\n",
        "print(\"Building Gradio interface...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "    # PDF Q&A Bot Using Hugging Face Open-Source Models\n",
        "    Ask questions about the document ('{pdf_filename}' if loaded, {len(pdf_text)} chars).\n",
        "    Select an open-source LLM to answer your question.\n",
        "    **Note:** Switching models takes time as the new model needs to be downloaded and loaded into the GPU.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store the pipeline in Gradio state for better practice (optional for this simple version)\n",
        "    # llm_pipeline_state = gr.State(None)\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=list(available_models.keys()),\n",
        "            label=\"🤖 Select LLM Model\",\n",
        "            value=list(available_models.keys())[0],  # Default to the first model\n",
        "        )\n",
        "        status_textbox = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "    question_textbox = gr.Textbox(\n",
        "        label=\"❓ Your Question\", lines=2, placeholder=\"Enter your question about the document here...\"\n",
        "    )\n",
        "    submit_button = gr.Button(\"Submit Question\", variant=\"primary\")\n",
        "    answer_textbox = gr.Textbox(label=\"💡 Answer\", lines=5, interactive=False)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    # When the dropdown changes, load the selected model\n",
        "    model_dropdown.change(\n",
        "        fn = load_llm_model,\n",
        "        inputs = [model_dropdown],\n",
        "        outputs = [status_textbox],  # Update status text. Ideally also update a gr.State for the pipeline\n",
        "        # outputs=[status_textbox, llm_pipeline_state] # If using gr.State\n",
        "    )\n",
        "\n",
        "    # When the button is clicked, call the submit handler\n",
        "    submit_button.click(\n",
        "        fn = handle_submit,\n",
        "        inputs = [question_textbox],\n",
        "        outputs = [answer_textbox],\n",
        "        # inputs=[question_textbox, llm_pipeline_state], # Pass state if using it\n",
        "    )\n",
        "\n",
        "    # --- Initial Model Load ---\n",
        "    # Easier: Manually load first model *before* launching Gradio for simplicity here\n",
        "    initial_model_name = list(available_models.keys())[0]\n",
        "    print(f\"Performing initial load of default model: {initial_model_name}...\")\n",
        "    status, _ = load_llm_model(initial_model_name)\n",
        "    status_textbox.value = status  # Set initial status\n",
        "    print(\"Initial load complete.\")\n",
        "\n",
        "\n",
        "# --- Launch the Gradio App ---\n",
        "print(\"Launching Gradio demo...\")\n",
        "demo.launch(debug=True)  # debug=True provides more detailed logs"
      ],
      "metadata": {
        "id": "MLiymGeRrArZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}