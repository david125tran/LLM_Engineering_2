{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f14oX0dJ7en9"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ---------------------------------- Libraries ----------------------------------\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q transformers accelerate bitsandbytes torch pypdf gradio\n",
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Imports ----------------------------------\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "from huggingface_hub import login, notebook_login\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sUpfAjT_7h-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Connection ----------------------------------\n",
        "hf_token = os.environ.get('HF_TOKEN') or userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"HuggingFace login successful.\")\n",
        "else:\n",
        "    print(\"HuggingFace token not found. Please set the HF_TOKEN environment variable or store it in Colab secrets.\")"
      ],
      "metadata": {
        "id": "qse4_vSi8EDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Use GPU ----------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    # Set default device to GPU\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    print(\"PyTorch default device set to CUDA (GPU).\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Running on CPU\")"
      ],
      "metadata": {
        "id": "-k2_jvrA8G9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Functions ----------------------------------\n",
        "# Helper function for markdown display\n",
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown.\"\"\"\n",
        "    display(Markdown(text))\n",
        "\n",
        "\n",
        "def combine_news_text(example):\n",
        "    # Handle potential None values gracefully\n",
        "    title = example.get(\"title\", \"\") or \"\"\n",
        "    description = example.get(\"description\", \"\") or \"\"\n",
        "\n",
        "    # Add a separator for clarity\n",
        "    return {\"full_text\": f\"Title: {title}\\nDescription: {description}\"}\n",
        "\n",
        "\n",
        "def format_model_output(output):\n",
        "    # Extract the content within <think> tags as Reason\n",
        "    reason_start = output.find(\"<think>\") + len(\"<think>\")\n",
        "    reason_end = output.find(\"</think>\")\n",
        "    reason = output[reason_start:reason_end].strip()\n",
        "\n",
        "    # Extract the content after </think> as Output\n",
        "    output_start = reason_end + len(\"</think>\")\n",
        "    model_output_content = output[output_start:].strip()\n",
        "\n",
        "    # Format the result\n",
        "    reason = f\"**Reason**:\\n{reason}\\n\"\n",
        "    output = f\"**Output**:\\n{model_output_content}\"\n",
        "    return reason, output\n",
        "\n",
        "\n",
        "def analyze_news_sentiment(news_text, llm_pipeline):\n",
        "    # Define the Prompt Template with specific instructions for the model\n",
        "    test_question = f\"\"\"You are a concise Financial News Analyst.\n",
        "    Analyze the provided news text. Output ONLY the three requested items below, each on a new line, prefixed with the specified label.\n",
        "\n",
        "    1.  REASONING: Brief financial reasoning (1-2 points max) for the sentiment.\n",
        "    2.  SENTIMENT: Choose ONE: Positive, Negative, Neutral.\n",
        "    3.  TAG: A concise topic tag (1-3 words).\n",
        "\n",
        "    Do NOT add any other text, greetings, or explanations.\n",
        "\n",
        "    News Text:\n",
        "    {news_text}\"\"\"\n",
        "\n",
        "    # Format the prompt according to DeepSeek's expected input format with thinking tags\n",
        "    test_prompt = f\"<|im_start|>user\\n{test_question}<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\"\n",
        "\n",
        "    # Print the prompt for debugging purposes to verify what's being sent to the model\n",
        "    print(f\"Test prompt:\\n{test_prompt}\")\n",
        "\n",
        "    # Run the model inference with specific generation parameters\n",
        "    outputs = llm_pipeline(\n",
        "        test_prompt,\n",
        "        max_new_tokens = 4000,\n",
        "        do_sample = True,\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.9)\n",
        "\n",
        "    # Extract the full generated text and parse it to separate reasoning from classification\n",
        "    # The format_model_output function likely separates the thinking process from the final answer\n",
        "    full_output = outputs[0][\"generated_text\"]\n",
        "    reason, output = format_model_output(full_output)\n",
        "\n",
        "    # Return both the reasoning process and the final sentiment classification\n",
        "    return reason, output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0fa_EcWv7t_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Load Dataset From HuggingFace ----------------------------------\n",
        "dataset_id = \"britneymuller/cnbc_newsfeed\"\n",
        "\n",
        "print(f\"Loading dataset from HuggingFace hub: {dataset_id}...\")\n",
        "\n",
        "news_dataset = load_dataset(dataset_id, split = \"train\")\n",
        "print(\"Dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "Kmyfosht8LSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Inspect Data ----------------------------------\n",
        "news_dataset\n",
        "print(\"\\n Dataset Features\")\n",
        "print(news_dataset.features)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "news_dataset.to_pandas()\n",
        "\n",
        "# Apply the `combine_news_text()` function to extract the 'title' and 'description' into 'full_text'\n",
        "news_dataset = news_dataset.map(combine_news_text)\n",
        "\n",
        "print(\"\\n--- Sample Data with 'full_text' ---\")\n",
        "print(news_dataset[0])\n",
        "\n",
        "# Display the full_text of the first sample\n",
        "print(news_dataset[0][\"full_text\"])"
      ],
      "metadata": {
        "id": "o-TI_CWw8kTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Dataset Features\n",
        "{'title': Value('string'), 'url': Value('string'), 'published_at': Value('string'), 'author': Value('string'), 'publisher': Value('string'), 'short_description': Value('string'), 'keywords': Value('string'), 'header_image': Value('string'), 'raw_description': Value('string'), 'description': Value('string'), 'scraped_at': Value('string')}\n",
        "\n",
        "Map: 100%\n",
        " 625/625 [00:00<00:00, 6349.94 examples/s]\n",
        "\n",
        "\n",
        "--- Sample Data with 'full_text' ---\n",
        "{'title': 'Santoli’s Wednesday market notes: Could September’s stock shakeout tee up strength for the fourth quarter?', 'url': 'https://www.cnbc.com/2021/09/29/santolis-wednesday-market-notes-could-septembers-stock-shakeout-tee-up-strength-for-the-fourth-quarter.html', 'published_at': '2021-09-29T17:09:39+0000', 'author': 'Michael Santoli', 'publisher': 'CNBC', 'short_description': \"This is the daily notebook of Mike Santoli, CNBC's senior markets commentator, with ideas about trends, stocks and market statistics.\", 'keywords': 'cnbc, Premium, Articles, Investment strategy, Markets, Investing, PRO Home, CNBC Pro, Pro: Santoli on Stocks, source:tagname:CNBC US Source', 'header_image': 'https://image.cnbcfm.com/api/v1/image/106949602-1632934577499-FINTECH_ETF_9-29.jpg?v=1632934691', 'raw_description': '<div class=\"group\"><p><em>This is the daily notebook of Mike Santoli, CNBC\\'s senior markets commentator, with ideas about trends, stocks and market statistics.</em></p><ul><li>A muted, inconclusive bounce that has left the indexes fully within yesterday\\'s low-to-high range all morning so far.</li></ul></div>', 'description': \"This is the daily notebook of Mike Santoli, CNBC's senior markets commentator, with ideas about trends, stocks and market statistics.A muted, inconclusive bounce that has left the indexes fully within yesterday's low-to-high range all morning so far.\", 'scraped_at': '2021-10-30 14:11:23.709372', 'full_text': \"Title: Santoli’s Wednesday market notes: Could September’s stock shakeout tee up strength for the fourth quarter?\\nDescription: This is the daily notebook of Mike Santoli, CNBC's senior markets commentator, with ideas about trends, stocks and market statistics.A muted, inconclusive bounce that has left the indexes fully within yesterday's low-to-high range all morning so far.\"}\n",
        "Title: Santoli’s Wednesday market notes: Could September’s stock shakeout tee up strength for the fourth quarter?\n",
        "Description: This is the daily notebook of Mike Santoli, CNBC's senior markets commentator, with ideas about trends, stocks and market statistics.A muted, inconclusive bounce that has left the indexes fully within yesterday's low-to-high range all morning so far.\n"
      ],
      "metadata": {
        "id": "qyl9MDxU90Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Open Source LLM ----------------------------------\n",
        "# Smaller DeepSeek model, that is suitable for Google Colab and reasoning tasks.\n",
        "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# AutoModelForCausalLM.from_pretrained(...): Loads a pre-trained language model for tasks like text generation (e.g., ChatGPT-like behavior).\n",
        "# model_id: The name or path of the model you want to load from Hugging Face (like \"meta-llama/Llama-3-8b\").\n",
        "# torch_dtype=\"auto\": Automatically chooses the best data type (like float16 or float32) based on your hardware for efficiency.\n",
        "# load_in_4bit=True: Loads the model in 4-bit precision to save memory and run on limited hardware like free Colab GPUs.\n",
        "# device_map=\"auto\": Automatically puts the model on the right device (GPU if available, otherwise CPU).\n",
        "# trust_remote_code=True: Use only with trusted models to avoid security risks.\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             torch_dtype = \"auto\",\n",
        "                                             load_in_4bit = True,\n",
        "                                             device_map = \"auto\",\n",
        "                                             trust_remote_code = True)\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded successfully in 4-bit!\")"
      ],
      "metadata": {
        "id": "WKokjBT1_1WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Load the Tokenizer ----------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "\n",
        "# ---------------------------------- Load the LLM Pipeline ----------------------------------\n",
        "llm_pipeline = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, torch_dtype = \"auto\", device_map = \"auto\")\n",
        "print(\"Text generation pipeline created successfully.\")\n",
        "\n",
        "# ---------------------------------- Prompt Engineering ----------------------------------\n",
        "test_question = \"Explain what electric cars are in simple terms. Keep the thinking short.\"\n",
        "test_prompt = f\"<|im_start|>user\\n{test_question}<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\"\n",
        "print(f\"Test prompt:\\n{test_prompt}\")"
      ],
      "metadata": {
        "id": "2tU1u_u9AEn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Test the LLM ----------------------------------\n",
        "outputs = llm_pipeline(test_prompt,\n",
        "                       max_new_tokens = 4000,\n",
        "                       do_sample = True,\n",
        "                       temperature = 0.7,\n",
        "                       top_p = 0.9)\n",
        "\n",
        "outputs"
      ],
      "metadata": {
        "id": "QZWcaKoMAXQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Query the LLM for Reason and Output ----------------------------------\n",
        "# Extract and print the generated text (only the response part)\n",
        "full_output = outputs[0][\"generated_text\"]\n",
        "reason, output = format_model_output(full_output)\n",
        "print_markdown(reason + output)"
      ],
      "metadata": {
        "id": "a0oNsm5LAiD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason:** Okay, so I need to explain what electric cars are in simple terms. Let me start by breaking down the term \"electric cars.\" I know that \"electric\" means something related to electricity, like the ability to use electricity for power. So, electric cars must have some form of electricity to drive.\n",
        "\n",
        "Now, what makes a car an electric car? I think it's about how the cars generate electricity. Maybe they use something like fuel cells or batteries. Fuel cells are like the inside of a battery, right? They use fuel to produce electricity. So, electric cars might have fuel cells instead of internal combustion engines.\n",
        "\n",
        "I should also consider the technology involved. I've heard that some electric cars use superconductors, which are materials that conduct electricity without resistance. This might be part of the battery system. Maybe they have a lot of superconductors to store energy efficiently.\n",
        "\n",
        "Another point is charging. Electric cars probably have an efficient charging system. If they use superconductors, it makes sense that they can charge quickly. But what about the battery capacity? They must have enough storage to keep the car moving for a while, even with low fuel levels.\n",
        "\n",
        "I should also mention the benefits. Electric cars can be more efficient because they don't rely on internal combustion engines. They're also more environmentally friendly. Plus, they might offer features like automatic driving or even regenerative braking, which helps the car recover some energy when it stops.\n",
        "\n",
        "Wait, what about the materials used? I think the cars are made from high-quality materials to reduce environmental impact. Maybe they're using recycled materials or have advanced engineering to keep costs low while being eco-friendly.\n",
        "\n",
        "I should structure this in simple terms. Maybe start by saying electric cars are like cars that use electricity to power themselves. Then explain the technology, like fuel cells and superconductors. Mention the benefits like efficiency, environment, and features. Keep it short and easy to understand.\n",
        "\n",
        "I think that covers it. Now, I'll put it together in a clear, concise way. **Output:** Electric cars are like cars that use electricity to power themselves. Instead of using fuel, they use something called fuel cells to generate electricity. These cars might have special materials like superconductors to store energy efficiently. They're more efficient and better for the environment. They might even have features like automatic driving or regenerative braking. All this makes electric cars a great choice for those who like being clean and efficient.\n"
      ],
      "metadata": {
        "id": "FeXXpHgnBDeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- LLM Reasoning and Output ----------------------------------\n",
        "# Print a separator line for clarity in the output\n",
        "print(\"\\n\" + \"=\" * 30 + \" TESTING ANALYSIS \" + \"=\" * 30)\n",
        "\n",
        "\n",
        "# Select a few random indices from the news dataset to test the analysis function\n",
        "random_indices = random.sample(range(len(news_dataset)), 3)\n",
        "\n",
        "# Iterate over each randomly selected index\n",
        "for index in random_indices:\n",
        "    # Retrieve the full text of the news item at the current index\n",
        "    sample_news = news_dataset[index][\"full_text\"]\n",
        "\n",
        "    # Analyze the sentiment of the sample news using the sentiment analysis function\n",
        "    reason, output = analyze_news_sentiment(sample_news, llm_pipeline)\n",
        "\n",
        "    # Print the analysis result header for the current index\n",
        "    print(f\"\\n--- Analysis Result for Index {index} ---\")\n",
        "\n",
        "    # Display the reasoning and output in a formatted markdown style\n",
        "    print_markdown(f\"{reason}\\n\\n{output}\")\n",
        "\n",
        "    # Print a separator line for better readability between results\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "IkxqeCNxI3tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Gradio Functions ----------------------------------\n",
        "def get_random_news():\n",
        "    \"\"\"Fetches and returns the full_text of a random news item.\"\"\"\n",
        "    if not news_dataset:  # Check if the news dataset is empty\n",
        "        return \"Error: No news items available.\"  # Return an error message if no news items are found\n",
        "    random_index = random.randint(0, len(news_dataset) - 1)  # Generate a random index to select a news item\n",
        "    news_text = news_dataset[random_index]['full_text']  # Retrieve the full text of the news item at the random index\n",
        "    print(f\"Fetched news item at index {random_index}\")  # Print the index of the fetched news item\n",
        "    return news_text  # Return the fetched news text\n",
        "\n",
        "def perform_analysis(news_text):\n",
        "    \"\"\"Triggers analysis on the provided news text.\"\"\"\n",
        "    if not news_text or news_text.startswith(\"Error\"):  # Check if the news text is empty or an error message\n",
        "        return \"Error: No news text to analyze.\"  # Return an error message if no valid news text is provided\n",
        "    print(f\"Analyzing news: {news_text[:50]}...\")  # Print the first 50 characters of the news text being analyzed\n",
        "    reason, output = analyze_news_sentiment(news_text, llm_pipeline)  # Analyze the sentiment of the news text\n",
        "    return reason, output  # Return the reasoning and output from the analysis"
      ],
      "metadata": {
        "id": "xiXlRdfz888g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Gradio UI ----------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Glass()) as demo:  # Create a Gradio Blocks interface with a glass theme\n",
        "    gr.Markdown(\"\"\"\n",
        "    # DeepSeek Financial News Analyzer\n",
        "    Fetches a random news item from the dataset.\n",
        "    Click 'Analyze News' to get sentiment classification and the model's reasoning.\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():  # Create a row for buttons\n",
        "        btn_fetch = gr.Button(\"🔄 Fetch Random News Item\")  # Button to fetch a random news item\n",
        "        btn_analyze = gr.Button(\"💡 Analyze News\", variant=\"primary\")  # Button to analyze the news\n",
        "\n",
        "    news_display = gr.Textbox(  # Textbox to display the news item\n",
        "        label=\"📰 News Text (Title & Description)\",  # Label for the textbox\n",
        "        lines=8,  # Number of lines in the textbox\n",
        "        interactive=False,  # Make the textbox non-interactive\n",
        "        placeholder=\"Click 'Fetch Random News Item' to display news.\"  # Placeholder text\n",
        "    )\n",
        "    # Creates a collapsible panel\n",
        "    with gr.Accordion(\"🤖 Model Reason\", open=True):  # Accordion for model reasoning\n",
        "        analysis_display = gr.Markdown()  # Markdown display for the reasoning\n",
        "\n",
        "    with gr.Accordion(\"🤖 Model Output\", open=True):  # Accordion for model output\n",
        "        analysis_output = gr.Markdown()  # Markdown display for the output\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    btn_fetch.click(  # Set up click event for the fetch button\n",
        "        fn=get_random_news,  # Function to call when the button is clicked\n",
        "        inputs=[],  # No inputs needed\n",
        "        outputs=[news_display]  # Output to the news display textbox\n",
        "    )\n",
        "\n",
        "    btn_analyze.click(  # Set up click event for the analyze button\n",
        "        fn=perform_analysis,  # Function to call when the button is clicked\n",
        "        inputs=[news_display],  # Input from the news display textbox\n",
        "        outputs=[analysis_display, analysis_output]  # Outputs to the reasoning and output displays\n",
        "    )\n",
        "\n",
        "    # Load initial news item when the app starts\n",
        "    demo.load(  # Load function to run when the app starts\n",
        "        fn=get_random_news,  # Function to call to get a random news item\n",
        "        inputs=None,  # No inputs needed\n",
        "        outputs=[news_display]  # Output to the news display textbox\n",
        "    )\n",
        "\n",
        "# --- Launch the Gradio App ---\n",
        "print(\"Launching Gradio demo...\")  # Print message indicating the app is launching\n",
        "demo.launch(debug=True)  # Launch the Gradio app in debug mode"
      ],
      "metadata": {
        "id": "1H1ExroKLj0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}