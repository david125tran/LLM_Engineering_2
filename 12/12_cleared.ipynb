{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALiIVrpkHW-d"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------- Libraries ----------------------------------\n",
        "!pip install -q transformers accelerate torch datasets peft trl openai scikit-learn gradio\n",
        "!pip install -U huggingface_hub\n",
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------------- Imports ----------------------------------\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import gc\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import create_repo, login, HfApi, notebook_login\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "import torch\n",
        "import trl\n",
        "from tqdm.notebook import tqdm\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from trl import SFTTrainer\n"
      ],
      "metadata": {
        "id": "FeZEka4vHwF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- HuggingFace Connection ----------------------------------\n",
        "hf_token = os.environ.get('HF_TOKEN') or userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "    print(\"HuggingFace login successful.\")\n",
        "else:\n",
        "    print(\"HuggingFace token not found. Please set the HF_TOKEN environment variable or store it in Colab secrets.\")"
      ],
      "metadata": {
        "id": "IuXLDDxBI6j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Use GPU ----------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    # Set default device to GPU\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    print(\"PyTorch default device set to CUDA (GPU).\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Running on CPU\")"
      ],
      "metadata": {
        "id": "8vEc2ujJIS6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Functions ----------------------------------\n",
        "# Helper function for markdown display\n",
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown.\"\"\"\n",
        "    display(Markdown(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "PnTtS4TFI8Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Load Dataset ----------------------------------\n",
        "# https://huggingface.co/datasets/Daniel-ML/sentiment-analysis-for-financial-news-v2/viewer\n",
        "dataset_id = \"Daniel-ML/sentiment-analysis-for-financial-news-v2\"\n",
        "\n",
        "# Load the data set\n",
        "labeled_dataset = load_dataset(dataset_id, split = \"train\")\n",
        "print(\"Dataset loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "8Pt8rWC2JH0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- View Dataset ----------------------------------\n",
        "# View the data set\n",
        "print(\"\\n--- Dataset Information ---\")\n",
        "print(labeled_dataset)\n",
        "\n",
        "# View the dataset features\n",
        "print(\"\\n--- Dataset Features ---\")\n",
        "print(labeled_dataset.features)\n",
        "\n",
        "# Unique Labels\n",
        "labels = labeled_dataset.to_pandas()['sentiment'].unique().tolist()\n",
        "print(\"\\n--- Unique Labels ---\")\n",
        "print(f\"Unique labels in the dataset: {labels}\")"
      ],
      "metadata": {
        "id": "iIqN04K2JN8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "--- Dataset Information ---  \n",
        "Dataset({  \n",
        "    features: ['sentiment', 'text'],  \n",
        "    num_rows: 4846  \n",
        "})  \n",
        "  \n",
        "--- Dataset Features ---  \n",
        "{'sentiment': Value('string'), 'text': Value('string')}  \n",
        "  \n",
        "--- Unique Labels ---  \n",
        "Unique labels in the dataset: ['neutral', 'negative', 'positive']  \n"
      ],
      "metadata": {
        "id": "I9YwftPuNa3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the data as a python DataFrame\n",
        "display(labeled_dataset.select(range(5)).to_pandas()[[\"text\", \"sentiment\"]])"
      ],
      "metadata": {
        "id": "ji663-UaN3qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text**  **sentiment**  \n",
        "**0**  According to Gran , the company has no plans t...  neutral  \n",
        "**1**  Technopolis plans to develop in stages an area...  neutral  \n",
        "**2**  The international electronic industry company ...  negative  \n",
        "**3**  With the new production plant the company woul...  positive  \n",
        "**4**  According to the company 's updated strategy f...  positive  "
      ],
      "metadata": {
        "id": "lD51Bd5iN87o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Split Dataset into Train / Test Dataset ----------------------------------\n",
        "# Split the Data into Train and Test Sets ---\n",
        "print(\"\\nSplitting data into Train (90%) and Test (10%)...\")\n",
        "train_test_split_ratio = 0.10\n",
        "\n",
        "# Set seed for reproducability\n",
        "seed = 42\n",
        "\n",
        "# Using datasets built-in method.  Shuffle the data to allow the LLM not learn the order of each sample.\n",
        "split_dataset = labeled_dataset.train_test_split(\n",
        "    test_size = train_test_split_ratio, seed = seed, shuffle = True,\n",
        ")\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "print(\"\\nTrain/Test Split Complete.\")"
      ],
      "metadata": {
        "id": "YO0q0NKaOVB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Splitting data into Train (90%) and Test (10%)...  \n",
        "Training set size: 4361  \n",
        "Test set size: 485  \n",
        "  \n",
        "Train/Test Split Complete.  \n"
      ],
      "metadata": {
        "id": "qXp1QMWNOmv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- View Dataset Balance ----------------------------------\n",
        "sns.countplot(labeled_dataset[\"sentiment\"])"
      ],
      "metadata": {
        "id": "T-toC8OwLOI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset is balanced, the model may have a bias towards certain data."
      ],
      "metadata": {
        "id": "ySBsMeX6Ln_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Format for Supervised Fine-Tuning (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "def format_for_sft_gemma(example, tokenizer):\n",
        "\n",
        "    # Define the conversation structure\n",
        "    system_prompt = \"Classify the sentiment of the following sentence from News as positive, negative, or neutral.\"\n",
        "    user_prompt = f\"Sentence: {example['text']}\"\n",
        "    assistant_response = example['sentiment'] # The target label\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{system_prompt}\\n{user_prompt}\"}, # Combine system/user for simplicity here\n",
        "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
        "    ]\n",
        "    # Apply the tokenizer's chat template.\n",
        "    # tokenize = False: means we want the text output, not token IDs.\n",
        "    # add_generation_prompt = False: means weâ€™re NOT adding the assistant prompt to generate the response. We already provided the assistant's message\n",
        "    formatted_text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False)\n",
        "    return {\"text\": formatted_text}"
      ],
      "metadata": {
        "id": "VUalnbacLsvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Test the Supervised Fine-Tuning Function (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "os_model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "# Define the quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # Use 4-bit weights (saves lots of RAM)\n",
        "    bnb_4bit_quant_type = \"nf4\", # nf4 is a better 4-bit format (Non-Float 4)\n",
        "    bnb_4bit_compute_dtype = torch.float16 # Math is done in float16 for speed\n",
        ")\n",
        "\n",
        "# Load the tokenizer to convert text to tokens (numbers)\n",
        "base_os_tokenizer = AutoTokenizer.from_pretrained(os_model_id)\n",
        "\n",
        "# Set pad token if missing (Gemma often doesn't have one)\n",
        "# A pad token is a special token used to make all input sequences the same length in a batch.\n",
        "if base_os_tokenizer.pad_token is None:\n",
        "    base_os_tokenizer.pad_token = base_os_tokenizer.eos_token\n",
        "    print(f\"Set pad_token to eos_token ({base_os_tokenizer.eos_token})\")\n"
      ],
      "metadata": {
        "id": "7338UQifM0wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Test Random Data (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "# Sample input dictionary\n",
        "# <bos>                      â†’ Beginning of sequence (tells the model: \"Start reading\")\n",
        "# <start_of_turn>user        â†’ Start of the user's turn\n",
        "# Classify the sentiment...  â†’ Instruction and user input\n",
        "# <end_of_turn>              â†’ End of user input\n",
        "# <start_of_turn>model       â†’ Start of the assistant/model's reply\n",
        "# positive                   â†’ The expected response\n",
        "# <end_of_turn>              â†’ End of assistant reply\n",
        "\n",
        "example = {\n",
        "    \"text\": \"The economy is showing signs of recovery after a tough year.\",\n",
        "    \"sentiment\": \"positive\"\n",
        "}\n",
        "\n",
        "# Apply the formatting function\n",
        "formatted_example = format_for_sft_gemma(example, base_os_tokenizer)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\n--- Sample Formatted Prompt ---\")\n",
        "print(formatted_example[\"text\"])"
      ],
      "metadata": {
        "id": "mdqHTjF6NgNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Format for Supervised Fine-Tuning (\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\") ----------------------------------\n",
        "def format_for_sft_deepseek_qwen(example, tokenizer):\n",
        "    # Target task\n",
        "    system_prompt = \"Classify the sentiment of the following sentence from News as positive, negative, or neutral.\"\n",
        "    user_prompt = f\"Sentence: {example['text']}\"\n",
        "    assistant_response = example['sentiment']  # gold label\n",
        "\n",
        "    # Qwen-style roles (ChatML)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_response},\n",
        "    ]\n",
        "\n",
        "    # For SFT we want the full conversation (incl. assistant target) serialized as text\n",
        "    # so we set tokenize=False and add_generation_prompt=False.\n",
        "    formatted_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "    return {\"text\": formatted_text}"
      ],
      "metadata": {
        "id": "eaEMmuquUJzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Test the Supervised Fine-Tuning Function (\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\") ----------------------------------\n",
        "os_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Define the quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # Use 4-bit weights (saves lots of RAM)\n",
        "    bnb_4bit_quant_type = \"nf4\", # nf4 is a better 4-bit format (Non-Float 4)\n",
        "    bnb_4bit_compute_dtype = torch.float16 # Math is done in float16 for speed\n",
        ")\n",
        "\n",
        "# Load the tokenizer to convert text to tokens (numbers)\n",
        "base_os_tokenizer = AutoTokenizer.from_pretrained(os_model_id)\n",
        "\n",
        "# Set pad token if missing (Gemma often doesn't have one)\n",
        "# A pad token is a special token used to make all input sequences the same length in a batch.\n",
        "if base_os_tokenizer.pad_token is None:\n",
        "    base_os_tokenizer.pad_token = base_os_tokenizer.eos_token\n",
        "    print(f\"Set pad_token to eos_token ({base_os_tokenizer.eos_token})\")\n"
      ],
      "metadata": {
        "id": "w0SyXeqxT_b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Test A Random Row (\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\") ----------------------------------\n",
        "# Pick a random index\n",
        "idx = random.randint(0, len(train_dataset) - 1)\n",
        "\n",
        "# Retrieve that row\n",
        "random_row = train_dataset[idx]\n",
        "\n",
        "print(random_row)\n",
        "\n",
        "example_2 = {\n",
        "    \"text\": random_row['text'],\n",
        "    \"sentiment\": random_row['sentiment']\n",
        "}\n",
        "\n",
        "# Apply the formatting function\n",
        "formatted_example_2 = format_for_sft_deepseek_qwen(example_2, base_os_tokenizer)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\n--- Sample Formatted Prompt ---\")\n",
        "print(formatted_example_2[\"text\"])"
      ],
      "metadata": {
        "id": "coNBUJiBSlgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Zero Shot Classification (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "os_model_id = \"google/gemma-3-1b-it\"\n",
        "\n",
        "base_os_tokenizer = AutoTokenizer.from_pretrained(os_model_id, use_fast=True)\n",
        "if base_os_tokenizer.pad_token is None:\n",
        "    base_os_tokenizer.pad_token = base_os_tokenizer.eos_token\n",
        "\n",
        "base_os_model = AutoModelForCausalLM.from_pretrained(\n",
        "    os_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "base_os_model.config.pad_token_id = base_os_tokenizer.pad_token_id\n",
        "print(\"Loaded Gemma-3 1B-IT without bnb 4-bit.\")\n"
      ],
      "metadata": {
        "id": "-xs2AaBwbqPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Apply Formatting for SFT Function to Test/Train Data (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "print(\"\\nFormatting data for SFTTrainer (Gemma format)...\")\n",
        "sft_train_dataset = train_dataset.map(\n",
        "              format_for_sft_gemma,\n",
        "              fn_kwargs={\"tokenizer\": base_os_tokenizer},\n",
        "              remove_columns=list(train_dataset.features)\n",
        "              )\n",
        "sft_test_dataset = test_dataset.map(\n",
        "              format_for_sft_gemma,\n",
        "              fn_kwargs={\"tokenizer\": base_os_tokenizer},\n",
        "              remove_columns=list(test_dataset.features)\n",
        "              )\n",
        "print(\"SFTTrainer (Gemma) formatting complete.\")\n",
        "print(\"Sample SFT Gemma format:\")\n",
        "print(sft_train_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "6yD1ZFmVdrUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Zero-Shot Classification Prompt Function (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "def create_zeroshot_prompt_gemma(sentence, tokenizer):\n",
        "\n",
        "    \"\"\"Creates zero-shot prompt using Gemma chat template.\"\"\"\n",
        "    system_prompt = f\"Classify the sentiment of the following sentence from Financial News. Respond with ONLY ONE of the following labels: {', '.join(labels)}.\"\n",
        "    user_prompt = f\"Sentence: {sentence}\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{system_prompt}\\n{user_prompt}\"},\n",
        "    ]\n",
        "    # Apply template, add_generation_prompt=True adds the assistant turn marker\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "2PE9znwfeANN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Inference Function for Zero-Shot Classification (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "# This function runs inference using the prompt generated above and returns the predicted sentiment label.\n",
        "# It Gets the formatted prompt using the first function \"create_zeroshot_prompt_gemma\".\n",
        "# It then tokenizes the prompt and sends it to the appropriate device (e.g., GPU):\n",
        "def classify_zero_shot_os_gemma(sentence, model, tokenizer):\n",
        "\n",
        "    prompt = create_zeroshot_prompt_gemma(sentence, tokenizer)\n",
        "    inputs = tokenizer(prompt, return_tensors = \"pt\", truncation = True, max_length = 512).to(model.device)\n",
        "\n",
        "    eos_id = tokenizer.eos_token_id          # eos_token_id: end of sentence\n",
        "    pad_id = tokenizer.pad_token_id          # pad_token_id: padding token\n",
        "\n",
        "    # Run the model to generate output, we are not performing gradient tracking since it's only inference!\n",
        "    with torch.no_grad():\n",
        "\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens = 10, # Limits the number of tokens the model can generate\n",
        "          eos_token_id = eos_id,\n",
        "          pad_token_id = eos_id,\n",
        "          do_sample = False\n",
        "      )\n",
        "    # Extracts and decode the generated text\n",
        "    response_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # print(f\"Generated Text (Gemma): {response_text}\")\n",
        "\n",
        "    # Clean and Validate Output\n",
        "    labels = [\"neutral\", \"negative\", \"positive\"]\n",
        "\n",
        "    predicted_label = \"Unknown\"\n",
        "    for label_text in labels:\n",
        "        if label_text.lower() in response_text.lower(): # Simple check if label is present\n",
        "              predicted_label = label_text\n",
        "              break\n",
        "\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "    return predicted_label\n"
      ],
      "metadata": {
        "id": "EA0lDI2beF3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Extract True Labels (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "true_labels = []\n",
        "\n",
        "# Extract the ground truth (target output) which represents the True class\n",
        "true_labels = [ex['sentiment'] for ex in test_dataset]\n",
        "true_labels"
      ],
      "metadata": {
        "id": "mC-9BJBqfE_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Evaluate on Test Set (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "base_os_predictions = [] # Renaming for clarity if needed, but reusing is fine\n",
        "\n",
        "# Invoke the pre-trained Gemma LLM on all testing datasets\n",
        "for example in tqdm(test_dataset):\n",
        "    predicted_label = classify_zero_shot_os_gemma(example['text'], base_os_model, base_os_tokenizer)\n",
        "    base_os_predictions.append(predicted_label)"
      ],
      "metadata": {
        "id": "1FR-EIOMfMuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Examine Zero-Shot Evaluation Results (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "valid_indices = [i for i, p in enumerate(base_os_predictions) if p not in [\"Error\", \"Unknown\"]]\n",
        "filtered_preds = [base_os_predictions[i] for i in valid_indices]\n",
        "filtered_true = [true_labels[i] for i in valid_indices]\n",
        "\n",
        "accuracy = accuracy_score(filtered_true, filtered_preds)\n",
        "report = classification_report(filtered_true, filtered_preds, labels=labels, zero_division=0, target_names=labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "3aFOnQsRf0OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Visualize Confusion Matrix (\"google/gemma-3-1b-it\") ----------------------------------\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(filtered_true, filtered_preds, labels=labels)\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, cbar=False)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Base Gemma Model (Zero-Shot)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# This confusion matrix shows that the model wasn't the best at predicting the true classification"
      ],
      "metadata": {
        "id": "9c_Ue5p_hRZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- LoRA ----------------------------------\n",
        "# --- Prepare model for k-bit training (important for quantized models) ---\n",
        "base_os_model.gradient_checkpointing_enable()  # Saves memory during training\n",
        "prepared_model = prepare_model_for_kbit_training(base_os_model)\n",
        "print(\"Model prepared for k-bit training.\")\n",
        "\n",
        "print(prepared_model)\n",
        "\n",
        "\n",
        "# --- LoRA Configuration ---\n",
        "# Target modules often include query/key/value layers in self-attention blocks\n",
        "# This depends on the model architecture (use print(prepared_model) to inspect layers)\n",
        "# For Qwen-based models, common targets might be 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
        "# Let's start with a reasonable default set\n",
        "lora_config = LoraConfig(\n",
        "    r = 16,  # LoRA rank (dimension of adapter matrices). Higher rank = more parameters, potentially better fit but slower. 8, 16, 32 are common.\n",
        "    lora_alpha = 32,  # Scaling factor for LoRA weights (often 2*r).\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Modules to apply LoRA to.\n",
        "    lora_dropout = 0.05,  # Dropout probability for LoRA layers.\n",
        "    bias = \"none\",  # Usually set to 'none'.\n",
        "    task_type = \"CAUSAL_LM\",  # Task type for sequence generation.\n",
        ")\n",
        "print(\"LoRA Config created.\")\n",
        "\n",
        "# --- Apply PEFT to the model ---\n",
        "# This adds the small LoRA layers into the big model. During fine-tuning, only these are updated â€” saving a lot of compute and memory\n",
        "peft_model = get_peft_model(prepared_model, lora_config)\n",
        "\n",
        "\n",
        "\n",
        "# The percentage (often < 1%) is small because LoRA only introduces and trains the parameters within the small adapter layers (`q_proj`, `k_proj`, etc. in our config),\n",
        "# It does not train the original billions of parameters in the base model (which remain frozen).\n",
        "# Training far fewer parameters requires significantly less GPU memory (VRAM), making it feasible to fine-tune large models on hardware like Colab's T4 GPU.\n",
        "# It's also much faster than full fine-tuning.\n",
        "print(\"PEFT model created.\")\n",
        "peft_model.print_trainable_parameters()  # See how few parameters we're actually training!\n"
      ],
      "metadata": {
        "id": "Yp42jfZ7kHxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Training Arguments ----------------------------------\n",
        "output_dir = \"./sentiment_finetuned_adapter\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"epoch\",\n",
        "    # optim=\"paged_adamw_8bit\",\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=False,   # avoid pinning CUDA tensors\n",
        "    dataloader_num_workers=0,      # simplest/safer for Colab-style envs\n",
        ")\n",
        "\n",
        "print(\"Training Arguments set.\")\n",
        "\n",
        "# ---------------- Safety: ensure tokenizer & model padding are set ----------------\n",
        "if getattr(base_os_tokenizer, \"pad_token\", None) is None:\n",
        "    base_os_tokenizer.pad_token = base_os_tokenizer.eos_token\n",
        "\n",
        "if hasattr(peft_model, \"config\"):\n",
        "    peft_model.config.pad_token_id = base_os_tokenizer.pad_token_id\n",
        "    peft_model.config.eos_token_id = base_os_tokenizer.eos_token_id\n",
        "    if getattr(peft_model.config, \"use_cache\", None) is not None:\n",
        "        peft_model.config.use_cache = False\n",
        "\n",
        "# ---------------- Build a text-only data collator ----------------\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=base_os_tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# ---------------- Prepare / normalize the dataset ----------------\n",
        "TEXT_COL = \"text\"  # change if your column is different\n",
        "\n",
        "# 1) If it's a DatasetDict, pick the train split\n",
        "if isinstance(sft_train_dataset, DatasetDict):\n",
        "    if \"train\" not in sft_train_dataset:\n",
        "        raise ValueError(f\"DatasetDict has splits {list(sft_train_dataset.keys())}, but no 'train' split.\")\n",
        "    sft_train_dataset = sft_train_dataset[\"train\"]\n",
        "\n",
        "# 2) If it's a list of dicts, wrap it\n",
        "if not hasattr(sft_train_dataset, \"column_names\"):\n",
        "    sft_train_dataset = Dataset.from_list(sft_train_dataset)\n",
        "\n",
        "assert TEXT_COL in sft_train_dataset.column_names, f\"Expected '{TEXT_COL}' column in dataset; found {sft_train_dataset.column_names}\"\n",
        "assert len(sft_train_dataset) > 0, \"Train dataset is empty.\"\n",
        "\n",
        "# ---------------- Tokenize ----------------\n",
        "MAX_LEN = 2048  # adjust to your context window & memory\n",
        "\n",
        "def _tok_fn(batch):\n",
        "    # Return CPU-side lists; collator will create tensors and pad dynamically\n",
        "    return base_os_tokenizer(\n",
        "        batch[TEXT_COL],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "tokenized_train = sft_train_dataset.map(\n",
        "    _tok_fn,\n",
        "    batched=True,\n",
        "    remove_columns=sft_train_dataset.column_names  # drop raw text + other cols\n",
        ")\n",
        "\n",
        "# Reset any prior \"torch\" formatting or CUDA device formatting on the dataset\n",
        "# so that we yield CPU-native python lists to the collator.\n",
        "if hasattr(tokenized_train, \"set_format\"):\n",
        "    tokenized_train.set_format(type=None)\n",
        "\n",
        "# Sanity checks\n",
        "required_cols = {\"input_ids\", \"attention_mask\"}\n",
        "missing = required_cols.difference(set(tokenized_train.column_names))\n",
        "if missing:\n",
        "    raise ValueError(f\"Tokenized dataset missing columns: {missing}. Columns present: {tokenized_train.column_names}\")\n",
        "if len(tokenized_train) == 0:\n",
        "    raise ValueError(\"Tokenized train dataset is empty after mapping.\")\n",
        "\n",
        "print(f\"Tokenized train samples: {len(tokenized_train)}\")\n",
        "\n",
        "# ---------------- Initialize vanilla HF Trainer (no TRL) ----------------\n",
        "from transformers import Trainer # Import Trainer class\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,                    # LoRA-wrapped model\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,       # <-- ensure this is a Dataset, not DatasetDict\n",
        "    tokenizer=base_os_tokenizer,         # OK despite deprecation; HF 5.x will prefer processing_class\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized (no SFTTrainer, no AutoProcessor).\")\n",
        "\n",
        "# ---------------- Train ----------------\n",
        "print(\"\\n--- Starting Fine-tuning... ---\")\n",
        "try:\n",
        "    training_results = trainer.train()\n",
        "    print(\"--- Fine-tuning Complete! ---\")\n",
        "    print(training_results)\n",
        "\n",
        "    # --- Save the LoRA Adapter + tokenizer ---\n",
        "    print(f\"Saving LoRA adapter model to {output_dir}...\")\n",
        "    trainer.save_model(output_dir)           # Saves adapter weights for PEFT-wrapped model\n",
        "    base_os_tokenizer.save_pretrained(output_dir)\n",
        "    print(\"Adapter and tokenizer saved.\")\n",
        "\n",
        "    # --- Clean up memory ---\n",
        "    del trainer\n",
        "    del peft_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Cleaned up training objects from memory.\")\n",
        "\n",
        "    fine_tuning_successful = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during fine-tuning: {e}\")\n",
        "    fine_tuning_successful = False"
      ],
      "metadata": {
        "id": "ILFF_8-hs8sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Push to HuggingFace ----------------------------------\n",
        "from huggingface_hub import create_repo, HfApi, HfFolder\n",
        "\n",
        "!huggingface-cli login\n",
        "\n",
        "model_name = \"gemma-sentiment-lora\"\n",
        "username = \"david125tran\"\n",
        "\n",
        "# Create or get your repo (wonâ€™t error if it already exists)\n",
        "repo_id = f\"{username}/{model_name}\"\n",
        "create_repo(repo_id, private=False, exist_ok=True)\n",
        "\n",
        "# Push the LoRA adapter + tokenizer folder\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"./sentiment_finetuned_adapter\",  # where your adapter and tokenizer are saved\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Upload Gemma 3B sentiment LoRA adapter ðŸŽ‰\",\n",
        ")\n",
        "\n",
        "print(f\"âœ… Successfully uploaded! View it here: https://huggingface.co/{repo_id}\")\n"
      ],
      "metadata": {
        "id": "z9yDCgNataR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- Evaluate Model ----------------------------------\n",
        "# ==== Config you may tweak ====\n",
        "os_model_id = os_model_id  # e.g. \"google/gemma-3-1b-it\"\n",
        "adapter_path = \"./sentiment_finetuned_adapter\"\n",
        "TEXT_COL = \"text\"\n",
        "labels = labels\n",
        "\n",
        "# ==== Imports ====\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# --------- Helper: pick best available quantization/backend ---------\n",
        "def build_base_model(model_id: str):\n",
        "    has_cuda = torch.cuda.is_available()\n",
        "    # try 4-bit\n",
        "    if has_cuda:\n",
        "        try:\n",
        "            import bitsandbytes as bnb  # noqa: F401\n",
        "            qconf = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            print(\"Loading base model in 4-bit quantizationâ€¦\")\n",
        "            return AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                quantization_config=qconf,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ 4-bit load failed: {e}\")\n",
        "            # try 8-bit\n",
        "            try:\n",
        "                print(\"Falling back to 8-bit quantizationâ€¦\")\n",
        "                qconf = BitsAndBytesConfig(load_in_8bit=True)\n",
        "                return AutoModelForCausalLM.from_pretrained(\n",
        "                    model_id,\n",
        "                    quantization_config=qconf,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                )\n",
        "            except Exception as e2:\n",
        "                print(f\"âš ï¸ 8-bit load failed: {e2}\")\n",
        "\n",
        "    # final fallback: full precision (bf16 if supported, else fp16/cpu)\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else (torch.float16 if has_cuda else torch.float32)\n",
        "    device_map = \"auto\" if has_cuda else None\n",
        "    print(f\"Falling back to full precision dtype={dtype} (device_map={device_map})â€¦\")\n",
        "    return AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=dtype,\n",
        "        device_map=device_map,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "# --------- Load tokenizer (from your saved adapter dir) ---------\n",
        "print(f\"\\n--- Loading Fine-tuned Open Source Model from {adapter_path} ---\")\n",
        "tuned_os_tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
        "if getattr(tuned_os_tokenizer, \"pad_token\", None) is None and getattr(tuned_os_tokenizer, \"eos_token\", None) is not None:\n",
        "    tuned_os_tokenizer.pad_token = tuned_os_tokenizer.eos_token\n",
        "\n",
        "# --------- Load base model with best available quantization ---------\n",
        "base_model_reload = build_base_model(os_model_id)\n",
        "if tuned_os_tokenizer.pad_token_id is not None and hasattr(base_model_reload, \"config\"):\n",
        "    base_model_reload.config.pad_token_id = tuned_os_tokenizer.pad_token_id\n",
        "    if getattr(base_model_reload.config, \"use_cache\", None) is not None:\n",
        "        base_model_reload.config.use_cache = False  # avoids gradient checkpointing warnings if enabled elsewhere\n",
        "\n",
        "# --------- Load PEFT adapter (no merge) ---------\n",
        "tuned_os_model = PeftModel.from_pretrained(base_model_reload, adapter_path)\n",
        "tuned_os_model.eval()\n",
        "print(\"âœ… Loaded PEFT model (base + adapter).\")\n",
        "\n",
        "# --------- Ensure ground-truth labels exist ---------\n",
        "if \"true_labels\" not in locals() or not true_labels:\n",
        "    print(\"Warning: true_labels not found from previous steps. Re-extracting from test_datasetâ€¦\")\n",
        "    true_labels = [ex[\"sentiment\"] for ex in test_dataset]\n",
        "\n",
        "# --------- Evaluation loop (no_grad + AMP where applicable) ---------\n",
        "tuned_os_predictions = []\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
        "\n",
        "autocast_dtype = torch.bfloat16 if use_bf16 else (torch.float16 if use_fp16 else None)\n",
        "autocast_ctx = (torch.autocast(device_type=\"cuda\", dtype=autocast_dtype) if autocast_dtype is not None\n",
        "                else torch.no_grad())  # will be replaced below for clarity\n",
        "\n",
        "print(\"\\n--- Evaluating fine-tuned model on test set ---\")\n",
        "with torch.no_grad():\n",
        "    if autocast_dtype is not None:\n",
        "        # AMP path\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=autocast_dtype):\n",
        "            for example in tqdm(test_dataset):\n",
        "                sentence = example[\"text\"]\n",
        "                pred = classify_zero_shot_os_gemma(sentence, tuned_os_model, tuned_os_tokenizer)\n",
        "                tuned_os_predictions.append(pred)\n",
        "    else:\n",
        "        # CPU / no AMP\n",
        "        for example in tqdm(test_dataset):\n",
        "            sentence = example[\"text\"]\n",
        "            pred = classify_zero_shot_os_gemma(sentence, tuned_os_model, tuned_os_tokenizer)\n",
        "            tuned_os_predictions.append(pred)\n",
        "\n",
        "# --------- Metrics ---------\n",
        "print(\"\\n--- Fine-Tuned Open Source Model: Evaluation Results ---\")\n",
        "valid_indices_tuned = [i for i, p in enumerate(tuned_os_predictions) if p not in [\"Error\", \"Unknown\"]]\n",
        "filtered_preds_tuned = [tuned_os_predictions[i] for i in valid_indices_tuned]\n",
        "filtered_true_tuned = [true_labels[i] for i in valid_indices_tuned]\n",
        "\n",
        "accuracy_tuned = accuracy_score(filtered_true_tuned, filtered_preds_tuned)\n",
        "report_tuned = classification_report(filtered_true_tuned, filtered_preds_tuned, labels=labels, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_tuned:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report_tuned)\n",
        "\n",
        "# If you computed base accuracy earlier as `accuracy`, compare:\n",
        "try:\n",
        "    print(\"\\n--- Comparison with Base OS Model ---\")\n",
        "    print(f\"Base OS Accuracy:       {accuracy:.4f}\")\n",
        "    print(f\"Fine-Tuned OS Accuracy: {accuracy_tuned:.4f}\")\n",
        "    improvement = accuracy_tuned - accuracy\n",
        "    print(f\"Improvement:            {improvement:+.4f}\")\n",
        "except NameError:\n",
        "    print(\"\\n(Base model accuracy `accuracy` not found; skipping comparison.)\")\n"
      ],
      "metadata": {
        "id": "7O6vkEu0w6dX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}